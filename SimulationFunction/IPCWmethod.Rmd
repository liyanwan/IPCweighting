```{r setup, include=FALSE}
rm(list = ls())
```

```{r}
true_survival_function <- function(dist,time,params = list()){
  # This function computes the BASELINE true survival probability for different survival distributions at a given time.
  # The supported distributions are exponential, Weibull, log-normal, and log-logistic.
  # Arguments:
  #   dist: A character string specifying the survival distribution to use. 
  #         Options of dist: c('exponential', 'Weibull', 'log-normal', or 'log-logistic')
  #   time: A numeric value representing the time point at which to compute the survival probability.
  #   params: A list of distribution-specific parameters.
  #           - For 'exponential', include 'lambda_base' (rate parameter).
  #           - For 'Weibull', include 'lambda_base' (1/(scale parameter)) and 'alpha' (shape parameter).
  #           - For 'log-normal', include 'mean' (mean of log-time) and 'sd' (standard deviation of log-time).
  #           - For 'log-logistic', include 'lambda_base' (1/(scale parameter)) and 'alpha' (shape parameter).
  #
  # Returns:
  #   A numeric value representing the survival probability at the given time.
  # Below is the formula of baseline survival function
  # Exponential S(t) = exp(-lambda*t)
  # Weibull S(t) = exp(-(lambda*t)^alpha)
  # Log Normal S(t) = 1-CDF((ln(t) - mean)/sd), CDF() here is the CDF of standard normal
  # Log Logistic S(t) = 1/(1+(lambda*t)^alpha)
    if (dist == "exponential") {
    return(exp(-params$lambda_base * time))
  } else if (dist == "Weibull") {
    return(exp(-(params$lambda_base * time)^params$alpha))
  } else if (dist == "log-normal") {
    return(plnorm(time, meanlog = params$mean, sdlog = params$sd, lower = FALSE))
  } else if (dist == "log-logistic") {
    return(1 / (1 +(params$lambda_base * time)^params$alpha))
  } else {
    stop("Error: Unsupported distribution.")
  }
}
```

```{r}
ols_error <- function(true_surv, est_surv) {
  return(sum((true_surv - est_surv)^2))
}

calibration_statistic <- function(type=c("uniform","quantile","random"), num_bins, predict_risk, observed_time, sigma, time_point) {
  predict_risk <- as.numeric(predict_risk)
  data = data.frame(predicted_risk_probability = predict_risk, observed_time = observed_time, sigma = sigma)
  data = data[order(data$predicted_risk_probability), ]
  # Create bins of data
  if(type=="uniform"){
    data$bin = cut(data$predicted_risk_probability, breaks = num_bins, labels = FALSE, include.lowest = TRUE)
  }
  else if(type == "quantile"){
    data$bin = cut(data$predicted_risk_probability,
                   breaks = quantile(data$predicted_risk_probability, probs = seq(0,1,by = 1/num_bins)),
                   labels = FALSE, include.lowest = TRUE)
  }
  else if(type == "random"){
    data$bin = sample(1:num_bins, length(data$predicted_risk_probability), replace = TRUE)
  }
  else{
    stop("Type can only be one of 'uniform', 'quantile', or 'random'")
  }
  K = 0
  avg_pred_probs = numeric(num_bins)
  observed_probs = numeric(num_bins)
  for (k in 1:num_bins) {
    bin_dt = data[data$bin == k, ]
    if (nrow(bin_dt) == 0) next
    # compute the average of predicted risk in bin k
    avg_pred_prob = mean(bin_dt$predicted_risk_probability)
    km_fit = survfit(Surv(observed_time, sigma) ~ 1, data = bin_dt)
    surv_summary = summary(km_fit, times = time_point,extend=TRUE)
    # compute the predicted risk using KM estimator
    bin_KM_pred = 1 - surv_summary$surv
    avg_pred_probs[k] = avg_pred_prob
    observed_probs[k] = bin_KM_pred
    num_at_risk = km_fit$n.risk[km_fit$time <= time_point]
    num_events = km_fit$n.event[km_fit$time <= time_point]
    if (length(num_at_risk) == 0 || any(num_at_risk <= 0)) {
        next
    }
    if (any(abs(num_at_risk - num_events) < 1e-6)) {
        next
    }
    greenwood_var = (surv_summary$surv)^2 * sum(num_events / (num_at_risk * (num_at_risk - num_events + 1e-5)))
    if (!is.finite(greenwood_var) || greenwood_var == 0) {
        next
    } else {
        K = K + ((avg_pred_prob - bin_KM_pred)^2 / greenwood_var)
    }
  }
  plot(observed_probs, avg_pred_probs-observed_probs, xlab = "observed Risk", ylab = "Pred - Observed Survival", 
       main = "Calibration Plot", pch = 19, col = "blue")
  abline(0, 0, col = "red", lty = 2)
  return(K)
}

c_index_censoring <- function(observed_time, sigma, predicted_risk) {
  # Arguments:
  # observed_time: vector of observed times (minimum(event_time, censor_time))
  # sigma: vector of event indicators (1 = event, 0 = censored)
  # predicted_risk: vector of predicted risks
  n = length(observed_time)
  concordant = 0
  comparable = 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      # Check if the pair can be compared
      if (((sigma[i] == 1) && (observed_time[i] < observed_time[j])) || ((sigma[j] == 1) && (observed_time[j] < observed_time[i]))) {
        comparable = comparable + 1
        if ((observed_time[i] < observed_time[j]) && (predicted_risk[i] > predicted_risk[j])) {
          concordant = concordant + 1
        } else if ((observed_time[j] < observed_time[i]) && (predicted_risk[j] > predicted_risk[i])) {
          concordant = concordant + 1
        }
      }
    }
  }
  c_index = concordant/comparable
  return(c_index)
}


weighted_loglikelihood <- function(test_IPCW, status, predicted_event_prob, penalized=FALSE, alpha =1, lambda=NULL, beta=NULL) {
  predicted_event_prob = pmax(pmin(predicted_event_prob, 1 - 1e-6), 1e-6)
  log_likelihood = -sum(
    ifelse(test_IPCW == 0, 
           0,
           test_IPCW*(status * log(predicted_event_prob) + (1-status)*log(1 - predicted_event_prob))
          )
  )
  if (!penalized) {
    return(log_likelihood)
  } else {
    if (is.null(lambda)||is.null(beta)) {
      stop("Lambda and beta must be provided for penalized log-likelihood.")
    }
    penalized_log_likelihood = log_likelihood - lambda * (alpha * sum(beta^2) + (1-alpha) * sum(abs(beta)))
    return(penalized_log_likelihood)
  }
}
```


```{r}
get_status <- function(observed_time, sigma, time_point){
  # Calculate the event status at `time_point`, commly denoted as E
  # Arguments:
  #   observed_time: Numeric vector of observed times.
  #   sigma: Numeric vector indicating censoring status (1: uncensored, 0: censored).
  #   time_point: Numeric value specifying the time point of interest.
  # Returns:
  #   A numeric vector of the same length as `observed_time`, where each element indicates the event status:
  #   - 1 if the event happened prior to time_point.
  #   - 0 if the event did not occur prior to time_point (the observation survived at time_point).
  #   - unknown if the observation is censored before time_point.

  E_obs <- ifelse(observed_time <= time_point & sigma == 1, 1, ifelse(observed_time > time_point, 0, NA))
  return(E_obs)
}
```

```{r}
get_cat_X <- function(X, num_obs, values, probs, name){
  # Generate a Categorical Covariate
  # This function adds a new categorical covariate to an existing dataframe. 
  # The covariate takes predefined values with specified probabilities.
  # Arguments:
  #   X: A dataframe containing all existing covariates.
  #   values: A vector specifying the possible categories (values) for the new covariate.
  #   probs: A numeric vector of probabilities corresponding to each value in 'values'. 
  #          The probabilities should sum to 1.
  #   name: A character string specifying the column name for the new categorical covariate.
  #
  # Returns:
  #   A dataframe with the new categorical covariate added as a column.
  if (length(values) != length(probs)) {
    stop("Error: 'values' and 'probs' must have the same length.")
  }
  if (sum(probs) != 1) {
    stop("Error: The probabilities 'probs' must sum to 1.")
  }
  partial <- sample(values, size = num_obs, replace = TRUE, prob = probs)
  if (ncol(X) == 0) {
    X <- data.frame(partial)
  } else {
    X <- cbind(X, partial)
  }
  colnames(X)[ncol(X)] <- name
  
  return(X)
}

get_cont_X <- function(X, distribution, num_obs, para1, para2, name) {
  # Generate a Continuous Covariate
  # This function adds a continuous covariate to an existing dataframe based on a specified distribution.
  # Arguments:
  #   X: A dataframe containing all existing covariates.
  #   distribution: A character string specifying the distribution to use for generating the covariate. 
  #                 Options are c('uniform', 'gaussian', or 'exponential')
  #   para1: The first parameter for the specified distribution. 
  #          For 'uniform', this is the lower bound
  #          for 'gaussian', this is the mean
  #          for 'exponential', this is the rate.
  #   para2: The second parameter for the specified distribution. 
  #          For 'uniform', this is the upper bound
  #          for 'gaussian', this is the standard deviation. 
  #          Not used for 'exponential'.
  #   name: A character string specifying the column name for the new continuous covariate.
  # Returns:
  #   A dataframe with the new continuous covariate added as a column.
  if (distribution == 'uniform') {
    if(para1<para2){
      partial = runif(num_obs, para1, para2)
    } else{
      stop("Error: 'para1' must be less than 'para2' for uniform distribution.")
    }
    mean <- (para2 + para1) / 2
    
  } else if (distribution == "gaussian") {
    if(para2 > 0){
      partial = rnorm(num_obs, para1, para2)
    } else{
      stop("Error: 'para2' (standard deviation) must be positive for Gaussian distribution.")
    }
    mean <- para1
    
  } else if (distribution == 'exponential') {
    if(para1 > 0){
      partial = rexp(num_obs, para1)
    } else{
      stop("Error: 'para1' (rate) must be positive for exponential distribution.")
    }
    mean <- 1 / para1
  } else {
    stop("Error: Unsupported distribution. Choose 'uniform', 'gaussian', or 'exponential'.")
  }
  if (ncol(X) == 0) {
    X <- data.frame(partial)
  } else {
    X <- cbind(X, partial)
  }
  colnames(X)[ncol(X)] = name
  mean_X <<- c(mean_X, mean)
  return(X)
}
```

```{r}
sim_censor_time <- function(dist = "exponential", n, params = list()){
  # Simulate censoring time
  # Arguments:
  #   dist: A character string specifying the distribution to use for simulating the censoring times.
  #         Options include 'exponential', 'Weibull', 'log-logistic', 'log-normal', and 'uniform'.
  #   n: An integer specifying the number of censoring times to generate.
  #   params: A list of distribution-specific parameters:
  #   - For 'exponential', include 'lambda' (rate parameter).
  #   - For 'Weibull', include 'lambda' (1/scale parameter) and 'alpha' (shape parameter).
  #   - For 'log-logistic', include 'lambda' (1/scale parameter) and 'alpha' (shape parameter).
  #   - For 'log-normal', include 'mean' (mean of the log-scale parameter) and 'sd' (standard deviation of the log-scale parameter).
  #   - For 'uniform', include 'start' (minimum value) and 'end' (maximum value).
  # Returns:
  #   A numeric vector of length n representing the simulated censoring times based on the specified distribution.
  if (dist == "exponential") {
    if (!is.null(params$lambda_base)) {
      return(rexp(n, rate = params$lambda_base))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "Weibull") {
    if (!is.null(params$lambda_base) & !is.null(params$alpha)) {
      return(rweibull(n, shape = params$alpha, scale = 1/params$lambda_base))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "log-logistic") {
    # Log-logistic distribution using qlogis for quantiles
    if (!is.null(params$lambda_base) & !is.null(params$alpha)) {
      u <- runif(n)
      return((1/(params$lambda_base))* (u / (1 - u))^(1 / params$alpha))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "log-normal") {
    if (!is.null(params$mean) & !is.null(params$sd)) {
      return(rlnorm(n, meanlog = params$mean, sdlog = params$sd))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "uniform"){
    if(!is.null(params$start) & !is.null(params$end)){
      censor_time = runif(n, params$start,params$end)
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else {
    stop("Unsupported distribution")
}
}
```

```{r}
sim_beta <- function(prop, time_point, mean_X){
  # Simulate Beta Coefficients for Covariates. This function estimates beta coefficients by solving an optimization problem. 
  # The goal is to find the beta coefficients such thatthe sum of the product of covariates and their respective beta values 
  # matches a specified constant.
  # Arguments:
  #   prop: A numeric value representing the survival proportion at the given time point.
  #   time_point: A numeric value representing the time point of interest.
  #   mean_X: A numeric vector representing the mean values of the covariates.
  # Returns:
  #   A numeric vector of the estimated beta coefficients.
  constant = -log(prop*time_point)
  objective_function <- function(beta, x, constant) {
    return((sum(x * beta) - constant)^2)
    }
  initial_beta = rep(1, length(mean_X))
  result = optim(initial_beta, objective_function, x = mean_X, constant = constant)
  beta_solution = result$par
  return(beta_solution)
}
```

```{r}
sim_T <- function(dist, n, params=list(), U_notuni){
  # Simulate event time based on baseline survival function.
  # Arguments:
  #   dist: A character string specifying the distribution for simulating event times.
  #         Options are c('exponential', 'Weibull', 'log-logistic', and 'log-normal')
  #   n: An integer specifying the number of event times to simulate.
  #   params: 
  #     - For 'exponential', include 'lambda_base' (rate parameter) and 'lc' (linear combination of covariates).
  #     - For 'Weibull', include 'lambda_base' (1/scale parameter), 'alpha' (shape parameter), and 'lc' (linear combination of covariates).
  #     - For 'log-logistic', include 'lambda_base' (1/scale parameter), 'alpha' (shape parameter), and 'lc' (linear combination of covariates).
  #     - For 'log-normal', include 'mean' (mean of the log-scale), 'sd' (standard deviation of the log-scale), and 'lc' (linear combination of covariates).
  # Returns:
  #   A numeric vector representing the simulated event times based on the specified distribution.
  
  # If baseline survival follows exponential distribution, event time is simulated directly
  # In other cases, simulate events times by suing the inverse of the cumulative hazard function (Using helper function simulate_U)
  if (dist == "exponential") {
    if (!is.null(params$lambda_base) & !is.null(params$lc)) {
      return(rexp(n, rate = (params$lambda_base*exp(params$lc))))
    }
} else if (dist == "Weibull") {
    if (!is.null(params$lambda_base) & !is.null(params$alpha)& !is.null(params$lc)) {
      U = runif(n, 0, 1)
    }
      return(1/params$lambda_base*(-log(U)/exp(params$lc))^(1/params$alpha))
  }
 else if (dist == "log-logistic") {
    if (!is.null(params$lambda_base) & !is.null(params$alpha)) {
      U = runif(n, 0, 1)
      return((1/(params$lambda_base)) * ((1-U^(1/exp(params$lc))) / U^(1/exp(params$lc)))^(1 / params$alpha))
    }
} else if (dist == "log-normal") {
    if (!is.null(params$mean) & !is.null(params$sd)& !is.null(params$lc)) {
      U = runif(n, 0, 1)
      return(exp(qnorm(1 - U^(1 / exp(params$lc))) * params$sd + params$mean))
    }
} else {
    stop("Unsupported distribution")
}
}
```

```{r}
Get_Ghat <-function(time_point, full_dt){
  # A helper function, using Kaplan Meier estimator of survival distribution of the censoring times.
  km_censor = survfit(Surv(observed_time, 1-sigma)~1, data = full_dt)
  step_function = stepfun(km_censor$time, c(1, km_censor$surv))
  return(step_function)
}
```

```{r}
km_surv_at_times <- function(observed_time, sigma, specific_times) {
  # A helper function to check if Ghat is correct
  data <- data.frame(time = observed_time, sigma = sigma)
  data <- data[order(data$time), ]  # Sort data by time
  survival_probs <- numeric(length(specific_times))
  for (j in seq_along(specific_times)) {
    specific_time <- specific_times[j]
    survival <- 1
    for (i in seq_len(nrow(data))) {
      if (data$time[i] > specific_time) {
        break
      }
      n_risk <- nrow(data) - i + 1  # Number at risk
      if (data$sigma[i] == 1) {
        survival <- survival * (n_risk - 1) / n_risk
      }
    }
    survival_probs[j] <- survival
  }
  return(survival_probs)
}
```

