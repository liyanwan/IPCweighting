```{r setup, include=FALSE}
rm(list = ls())
```

```{r}
true_survival_function <- function(dist,time,params = list()){
  # This function computes the BASELINE true survival probability for different survival distributions at a given time.
  # The supported distributions are exponential, Weibull, log-normal, and log-logistic.
  # Arguments:
  #   dist: A character string specifying the survival distribution to use. 
  #         Options of dist: c('exponential', 'Weibull', 'log-normal', or 'log-logistic')
  #   time: A numeric value representing the time point at which to compute the survival probability.
  #   params: A list of distribution-specific parameters.
  #           - For 'exponential', include 'lambda_base' (rate parameter).
  #           - For 'Weibull', include 'lambda_base' (1/(scale parameter)) and 'alpha' (shape parameter).
  #           - For 'log-normal', include 'mean' (mean of log-time) and 'sd' (standard deviation of log-time).
  #           - For 'log-logistic', include 'lambda_base' (1/(scale parameter)) and 'alpha' (shape parameter).
  #
  # Returns:
  #   A numeric value representing the survival probability at the given time.
  # Below is the formula of baseline survival function
  # Exponential S(t) = exp(-lambda*t)
  # Weibull S(t) = exp(-(lambda*t)^alpha)
  # Log Normal S(t) = 1-CDF((ln(t) - mean)/sd), CDF() here is the CDF of standard normal
  # Log Logistic S(t) = 1/(1+(lambda*t)^alpha)
    if (dist == "exponential") {
    return(exp(-params$lambda_base * time))
  } else if (dist == "Weibull") {
    return(exp(-(params$lambda_base * time)^params$alpha))
  } else if (dist == "log-normal") {
    return(exp(plnorm(time, meanlog = params$mean, sdlog = params$sd, lower = FALSE, log = TRUE)))
  } else if (dist == "log-logistic") {
    return(1 / (1 +(params$lambda_base * time)^params$alpha))
  } else {
    stop("Error: Unsupported distribution.")
  }
}
```

```{r}
bootstrap_sampling_var <- function(bin_dt, observed_time, sigma, time_point, num_bootstrap) {
  lst = numeric(num_bootstrap)
  for (b in 1:num_bootstrap) {
    bootstrap_sample = bin_dt[sample(nrow(bin_dt), replace = TRUE), ]
    km_fit = survfit(Surv(observed_time, sigma) ~ 1, data = bootstrap_sample)
    surv_summary = summary(km_fit, times = time_point)
    lst[b] = 1 - surv_summary$surv
  }
  print(lst)
  sampling_var = var(lst)
  print(sampling_var)
  return(sampling_var)
}
```

```{r}
ols_error <- function(true_surv, est_surv) {
  return(sum((true_surv - est_surv)^2))
}

calibration_statistic <- function(type=c("uniform","quantile","random"), num_bins, predict_risk, observed_time, sigma, time_point, num_bootstrap = 5) {
  predict_risk <- as.numeric(predict_risk)
  data = data.frame(predicted_risk_probability = predict_risk, observed_time = observed_time, sigma = sigma)
  data = data[order(data$predicted_risk_probability), ]
  # Create bins of data
  print(class(data$predicted_risk_probability))
  data$bin = cut(data$predicted_risk_probability, breaks = num_bins, labels = FALSE)
  View(data)
  K = 0
  for (k in 1:num_bins) {
    bin_dt = data[data$bin == k, ]
    print(1)
    avg_pred_prob = mean(bin_dt$predicted_risk_probability)
    print(avg_pred_prob)
    km_fit = survfit(Surv(observed_time, sigma) ~ 1, data = bin_dt)
    surv_summary = summary(km_fit, times = time_point)
    bin_KM_pred = 1 - surv_summary$surv
    KM_sampling_variance = bootstrap_sampling_var(bin_dt, 
                                                  bin_dt$observed_time, 
                                                  bin_dt$sigma, 
                                                  time_point, 
                                                  num_bootstrap)
    K = K + ((avg_pred_prob - bin_KM_pred)^2 / KM_sampling_variance)
  }
  return(K)
}

Cindex <- function(){
  
}

deviance_data <- function(status, predicted_probability){
  predicted_probability = pmax(pmin(predicted_probability, 1 - 1e-15), 1e-15)
  return(sum(status*log(predicted_probability)+(1-status)*log(1-predicted_probability)))
}
```

```{r}
get_status <- function(observed_time, sigma, time_point){
  # Calculate the event status at `time_point`, commly denoted as E
  # Arguments:
  #   observed_time: Numeric vector of observed times.
  #   sigma: Numeric vector indicating censoring status (1: uncensored, 0: censored).
  #   time_point: Numeric value specifying the time point of interest.
  # Returns:
  #   A numeric vector of the same length as `observed_time`, where each element indicates the event status:
  #   - 1 if the event happened prior to time_point.
  #   - 0 if the event did not occur prior to time_point (the observation survived at time_point).
  #   - unknown if the observation is censored before time_point.

  E_obs <- ifelse(observed_time <= time_point & sigma == 1, 1, ifelse(observed_time > time_point, 0, NA))
  return(E_obs)
}
```

```{r}
get_cat_X <- function(X, num_obs, values, probs, name){
  # Generate a Categorical Covariate
  # This function adds a new categorical covariate to an existing dataframe. 
  # The covariate takes predefined values with specified probabilities.
  # Arguments:
  #   X: A dataframe containing all existing covariates.
  #   values: A vector specifying the possible categories (values) for the new covariate.
  #   probs: A numeric vector of probabilities corresponding to each value in 'values'. 
  #          The probabilities should sum to 1.
  #   name: A character string specifying the column name for the new categorical covariate.
  #
  # Returns:
  #   A dataframe with the new categorical covariate added as a column.
  if (length(values) != length(probs)) {
    stop("Error: 'values' and 'probs' must have the same length.")
  }
  if (sum(probs) != 1) {
    stop("Error: The probabilities 'probs' must sum to 1.")
  }
  partial <- sample(values, size = num_obs, replace = TRUE, prob = probs)
  if (ncol(X) == 0) {
    X <- data.frame(partial)
  } else {
    X <- cbind(X, partial)
  }
  colnames(X)[ncol(X)] <- name
  
  return(X)
}

get_cont_X <- function(X, distribution, num_obs, para1, para2, name) {
  # Generate a Continuous Covariate
  # This function adds a continuous covariate to an existing dataframe based on a specified distribution.
  # Arguments:
  #   X: A dataframe containing all existing covariates.
  #   distribution: A character string specifying the distribution to use for generating the covariate. 
  #                 Options are c('uniform', 'gaussian', or 'exponential')
  #   para1: The first parameter for the specified distribution. 
  #          For 'uniform', this is the lower bound
  #          for 'gaussian', this is the mean
  #          for 'exponential', this is the rate.
  #   para2: The second parameter for the specified distribution. 
  #          For 'uniform', this is the upper bound
  #          for 'gaussian', this is the standard deviation. 
  #          Not used for 'exponential'.
  #   name: A character string specifying the column name for the new continuous covariate.
  # Returns:
  #   A dataframe with the new continuous covariate added as a column.
  if (distribution == 'uniform') {
    if(para1<para2){
      partial = runif(num_obs, para1, para2)
    } else{
      stop("Error: 'para1' must be less than 'para2' for uniform distribution.")
    }
    mean <- (para2 + para1) / 2
    
  } else if (distribution == "gaussian") {
    if(para2 > 0){
      partial = rnorm(num_obs, para1, para2)
    } else{
      stop("Error: 'para2' (standard deviation) must be positive for Gaussian distribution.")
    }
    mean <- para1
    
  } else if (distribution == 'exponential') {
    if(para1 > 0){
      partial = rexp(num_obs, para1)
    } else{
      stop("Error: 'para1' (rate) must be positive for exponential distribution.")
    }
    mean <- 1 / para1
  } else {
    stop("Error: Unsupported distribution. Choose 'uniform', 'gaussian', or 'exponential'.")
  }
  if (ncol(X) == 0) {
    X <- data.frame(partial)
  } else {
    X <- cbind(X, partial)
  }
  colnames(X)[ncol(X)] = name
  mean_X <<- c(mean_X, mean)
  return(X)
}
```

```{r}
sim_censor_time <- function(dist = "exponential", n, params = list()){
  # Simulate censoring time
  # Arguments:
  #   dist: A character string specifying the distribution to use for simulating the censoring times.
  #         Options include 'exponential', 'Weibull', 'log-logistic', 'log-normal', and 'uniform'.
  #   n: An integer specifying the number of censoring times to generate.
  #   params: A list of distribution-specific parameters:
  #   - For 'exponential', include 'lambda' (rate parameter).
  #   - For 'Weibull', include 'lambda' (1/scale parameter) and 'alpha' (shape parameter).
  #   - For 'log-logistic', include 'lambda' (1/scale parameter) and 'alpha' (shape parameter).
  #   - For 'log-normal', include 'mean' (mean of the log-scale parameter) and 'sd' (standard deviation of the log-scale parameter).
  #   - For 'uniform', include 'start' (minimum value) and 'end' (maximum value).
  # Returns:
  #   A numeric vector of length n representing the simulated censoring times based on the specified distribution.
  if (dist == "exponential") {
    if (!is.null(params$lambda)) {
      return(rexp(n, rate = params$lambda))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "Weibull") {
    if (!is.null(params$lambda) & !is.null(params$alpha)) {
      return(rweibull(n, shape = params$alpha, scale = 1/params$lambda))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "log-logistic") {
    # Log-logistic distribution using qlogis for quantiles
    if (!is.null(params$lambda) & !is.null(params$alpha)) {
      u <- runif(n)
      return((1/(params$lambda))* (u / (1 - u))^(1 / params$alpha))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "log-normal") {
    if (!is.null(params$mean) & !is.null(params$sd)) {
      return(rlnorm(n, meanlog = params$mean, sdlog = params$sd))
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else if (dist == "uniform"){
    if(!is.null(params$start) & !is.null(params$end)){
      censor_time = runif(n, params$start,params$end)
    }
    else{
      stop("Please input the correct parameters.")
    }
  } 
  else {
    stop("Unsupported distribution")
}
}
```

```{r}
sim_beta <- function(prop, time_point, mean_X){
  # Simulate Beta Coefficients for Covariates. This function estimates beta coefficients by solving an optimization problem. 
  # The goal is to find the beta coefficients such thatthe sum of the product of covariates and their respective beta values 
  # matches a specified constant.
  # Arguments:
  #   prop: A numeric value representing the survival proportion at the given time point.
  #   time_point: A numeric value representing the time point of interest.
  #   mean_X: A numeric vector representing the mean values of the covariates.
  # Returns:
  #   A numeric vector of the estimated beta coefficients.
  constant = -log(prop*time_point)
  objective_function <- function(beta, x, constant) {
    return((sum(x * beta) - constant)^2)
    }
  initial_beta = rep(1, length(mean_X))
  result = optim(initial_beta, objective_function, x = mean_X, constant = constant)
  beta_solution = result$par
  return(beta_solution)
}
```

```{r}
simulate_U <- function(n) {
  # Simulate S(t|X_i) = U.
  # It is a helper function of sim_T below, using inverse cumulative function.
  # Arguments:
  # n: An integer specifying the number of values to simulate in the following ways:
  # U ~ unif(0.01,0.25) with probability 0.4
  # U ~ unif(0.25, 0.5) with probability 0.35
  # U ~ unif(0.5, 0.75] with probability 0.15
  # U ~ unif(0.75, 0.99) with probability 0.1
  # Returns:
  # A numeric vector where each element is sampled from a pievewise uniform distribution.
  
  simulated_values = numeric(n)
  probs = c(0.4, 0.35, 0.15, 0.1)
  intervals = matrix(c(0.01, 0.25, 0.25, 0.5, 0.5, 0.75, 0.75, 0.99), ncol = 2, byrow = TRUE)
  cum_probs = cumsum(probs)
    for (i in 1:n) {
      U = runif(1)
      if (U <= cum_probs[1]) {
        simulated_values[i]= runif(1, intervals[1, 1], intervals[1, 2]) 
        # Simulate from (0, 0.25]
      } else if (U <= cum_probs[2]) {
        simulated_values[i] = runif(1, intervals[2, 1], intervals[2, 2])  
        # Simulate from (0.25, 0.5]
      } else if (U <= cum_probs[3]) {
        simulated_values[i] = runif(1, intervals[3, 1], intervals[3, 2])  
        # Simulate from (0.5, 0.75]
      } else {
        simulated_values[i] = runif(1, intervals[4, 1], intervals[4, 2])  
        # Simulate from (0.75, 1]
      }
    }
  return(simulated_values)
}
```

```{r}
sim_T <- function(dist, n, params=list(), U_notuni){
  # Simulate event time based on baseline survival function.
  # Arguments:
  #   dist: A character string specifying the distribution for simulating event times.
  #         Options are c('exponential', 'Weibull', 'log-logistic', and 'log-normal')
  #   n: An integer specifying the number of event times to simulate.
  #   params: 
  #     - For 'exponential', include 'lambda_base' (rate parameter) and 'lc' (linear combination of covariates).
  #     - For 'Weibull', include 'lambda_base' (1/scale parameter), 'alpha' (shape parameter), and 'lc' (linear combination of covariates).
  #     - For 'log-logistic', include 'lambda_base' (1/scale parameter), 'alpha' (shape parameter), and 'lc' (linear combination of covariates).
  #     - For 'log-normal', include 'mean' (mean of the log-scale), 'sd' (standard deviation of the log-scale), and 'lc' (linear combination of covariates).
  #   U_notuni: A logical value indicating whether to simulate U from simulate_U (TRUE) or from a unif[0,1] (FALSE).
  # Returns:
  #   A numeric vector representing the simulated event times based on the specified distribution.
  
  # If baseline survival follows exponential distribution, event time is simulated directly
  # In other cases, simulate events times by suing the inverse of the cumulative hazard function (Using helper function simulate_U)
  if (dist == "exponential") {
    if (!is.null(params$lambda_base) & !is.null(params$lc)) {
      return(rexp(n, rate = (params$lambda_base*exp(params$lc))))
    }
} else if (dist == "Weibull") {
    if (!is.null(params$lambda_base) & !is.null(params$alpha)& !is.null(params$lc)) {
      U = simulate_U(n)
    }
      return(1/params$lambda_base*(-log(U)/exp(params$lc))^(1/params$alpha))
  }
 else if (dist == "log-logistic") {
    if (!is.null(params$lambda_base) & !is.null(params$alpha)) {
      U = simulate_U(n)
      return((1/(params$lambda_base)) * ((1-U^(1/exp(params$lc))) / U^(1/exp(params$lc)))^(1 / params$alpha))
    }
} else if (dist == "log-normal") {
    if (!is.null(params$mean) & !is.null(params$sd)& !is.null(params$lc)) {
      U = simulate_U(n)
      S_t = 1-U^(1 / exp(params$lc))
      return(exp(qnorm(1 - S_t) * params$sd + params$mean))
    }
} else {
    stop("Unsupported distribution")
}
}
```

```{r}
Get_Ghat <-function(time_point, full_dt){
  # A helper function, using Kaplan Meier estimator of survival distribution of the censoring times.
  km_censor = survfit(Surv(observed_time, 1-sigma)~1, data = full_dt)
  survest_Xi = stepfun(km_censor$time, c(1, km_censor$surv))
  return(survest_Xi)
}
```


